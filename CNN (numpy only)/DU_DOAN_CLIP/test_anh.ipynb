{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 93\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a numpy array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights\n\u001b[0;32m---> 93\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_weights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Danh sách các biển báo giao thông (label)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m traffic_sign_labels \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThang hoac Phai, cam queo Trai\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCam di nguoc chieu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m5\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuong phai di vung phai\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    107\u001b[0m }\n",
      "Cell \u001b[0;32mIn[16], line 80\u001b[0m, in \u001b[0;36mload_weights\u001b[0;34m(weights_dir)\u001b[0m\n\u001b[1;32m     78\u001b[0m byte_data \u001b[38;5;241m=\u001b[39m weight_data[key]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(byte_data, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     decoded_data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_data\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Giải mã từ byte\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(decoded_data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     82\u001b[0m         weights[key] \u001b[38;5;241m=\u001b[39m decoded_data\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# # Hàm tải trọng số từ các file .pth\n",
    "# def load_weights(weights_dir):\n",
    "#     weights = {}\n",
    "#     import os\n",
    "#     for weight_file in os.listdir(weights_dir):\n",
    "#         layer_name = weight_file.replace('.pth', '')\n",
    "#         weights[layer_name] = np.load(os.path.join(weights_dir, weight_file), allow_pickle=True)\n",
    "#     return weights\n",
    "\n",
    "# Hàm thực hiện convolution\n",
    "\n",
    "def conv2d(input, filters, bias, stride=1, padding=1):\n",
    "    c_out, c_in, kernel_size, _ = filters.shape\n",
    "    h, w = input.shape[1], input.shape[2]\n",
    "    \n",
    "    # Tính kích thước đầu ra\n",
    "    h_out = (h - kernel_size + 2 * padding) // stride + 1\n",
    "    w_out = (w - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "    # Padding input\n",
    "    input_padded = np.pad(input, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
    "    output = np.zeros((c_out, h_out, w_out))\n",
    "\n",
    "    # Tính convolution\n",
    "    for out_c in range(c_out):\n",
    "        for i in range(h_out):\n",
    "            for j in range(w_out):\n",
    "                region = input_padded[:, i*stride:i*stride+kernel_size, j*stride:j*stride+kernel_size]\n",
    "                output[out_c, i, j] = np.sum(region * filters[out_c]) + bias[out_c]\n",
    "\n",
    "    return output\n",
    "\n",
    "# Hàm kích hoạt ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Hàm max pooling\n",
    "def max_pool2d(input, pool_size=2, stride=2):\n",
    "    c, h, w = input.shape\n",
    "    h_out = (h - pool_size) // stride + 1\n",
    "    w_out = (w - pool_size) // stride + 1\n",
    "    output = np.zeros((c, h_out, w_out))\n",
    "\n",
    "    for i in range(h_out):\n",
    "        for j in range(w_out):\n",
    "            region = input[:, i*stride:i*stride+pool_size, j*stride:j*stride+pool_size]\n",
    "            output[:, i, j] = np.max(region, axis=(1, 2))\n",
    "    return output\n",
    "\n",
    "# Hàm flatten\n",
    "def flatten(x):\n",
    "    return x.reshape(-1)\n",
    "\n",
    "# Hàm fully connected (Dense layer)\n",
    "def dense(input, weights, bias):\n",
    "    return np.dot(weights, input) + bias\n",
    "\n",
    "# Tải cấu trúc và trọng số của mô hình\n",
    "with open('model_structure.json', 'r') as f:\n",
    "    model_structure = json.load(f)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Hàm tải trọng số mà không cần sử dụng pickle\n",
    "def load_weights(weights_dir):\n",
    "    weights = {}\n",
    "    for weight_file in os.listdir(weights_dir):\n",
    "        # Đọc trọng số từ tệp .npz hoặc các tệp numpy lưu trữ\n",
    "        weight_data = np.load(os.path.join(weights_dir, weight_file), allow_pickle=True)\n",
    "        \n",
    "        for key in weight_data.keys():\n",
    "            weights[key] = weight_data[key]  # Lưu trọng số trực tiếp\n",
    "\n",
    "    return weights\n",
    "\n",
    "weights = load_weights('model_weights')\n",
    "\n",
    "# Danh sách các biển báo giao thông (label)\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    7: 'Canh bao co tre em',\n",
    "    8: 'Cam dau xe',\n",
    "    4: 'Di cham thoi',\n",
    "    9: 'Cam dung va do xe',\n",
    "    6: 'Huong di theo vach ke duong',\n",
    "    3: 'Gap khuc phai',\n",
    "    5: 'Huong phai di vung phai',\n",
    "}\n",
    "\n",
    "# Hàm chuẩn bị dữ liệu\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize((64, 64))\n",
    "    image = np.asarray(image) / 255.0\n",
    "    image = (image - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    return image.transpose(2, 0, 1)  # Chuyển kênh thành (C, H, W)\n",
    "\n",
    "# Hàm dự đoán\n",
    "# Hàm dự đoán\n",
    "def predict_traffic_sign(image_path):\n",
    "    # Tiền xử lý ảnh\n",
    "    x = preprocess_image(image_path)\n",
    "\n",
    "    # Forward pass qua từng lớp\n",
    "    x = conv2d(x, weights['conv1.weight/data.pkl'], weights['conv1.bias/data.pkl'])\n",
    "    x = relu(x)\n",
    "    x = max_pool2d(x)\n",
    "\n",
    "    x = conv2d(x, weights['conv2.weight/data.pkl'], weights['conv2.bias/data.pkl'])\n",
    "    x = relu(x)\n",
    "    x = max_pool2d(x)\n",
    "\n",
    "    x = flatten(x)\n",
    "\n",
    "    x = dense(x, weights['fc1.weight/data.pkl'], weights['fc1.bias/data.pkl'])\n",
    "    x = relu(x)\n",
    "\n",
    "    x = dense(x, weights['fc2.weight/data.pkl'], weights['fc2.bias/data.pkl'])\n",
    "\n",
    "    # Lấy nhãn dự đoán\n",
    "    predicted_label_idx = np.argmax(x)\n",
    "    return traffic_sign_labels.get(predicted_label_idx, \"Unknown\")\n",
    "\n",
    "\n",
    "# Dự đoán\n",
    "image_path = 'a.png'\n",
    "predicted_label = predict_traffic_sign(image_path)\n",
    "print(f'Predicted Traffic Sign: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in weights: dict_keys(['fc1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'conv1.weight', 'fc2.weight', 'conv1.bias', 'fc2.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights = load_weights('model_weights')\n",
    "print(\"Available keys in weights:\", weights.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1.bias, Keys: dict_keys(['fc1.bias/data.pkl', 'fc1.bias/byteorder', 'fc1.bias/data/0', 'fc1.bias/version', 'fc1.bias/.data/serialization_id'])\n",
      "Layer: conv2.weight, Keys: dict_keys(['conv2.weight/data.pkl', 'conv2.weight/byteorder', 'conv2.weight/data/0', 'conv2.weight/version', 'conv2.weight/.data/serialization_id'])\n",
      "Layer: conv2.bias, Keys: dict_keys(['conv2.bias/data.pkl', 'conv2.bias/byteorder', 'conv2.bias/data/0', 'conv2.bias/version', 'conv2.bias/.data/serialization_id'])\n",
      "Layer: fc1.weight, Keys: dict_keys(['fc1.weight/data.pkl', 'fc1.weight/byteorder', 'fc1.weight/data/0', 'fc1.weight/version', 'fc1.weight/.data/serialization_id'])\n",
      "Layer: conv1.weight, Keys: dict_keys(['conv1.weight/data.pkl', 'conv1.weight/byteorder', 'conv1.weight/data/0', 'conv1.weight/version', 'conv1.weight/.data/serialization_id'])\n",
      "Layer: fc2.weight, Keys: dict_keys(['fc2.weight/data.pkl', 'fc2.weight/byteorder', 'fc2.weight/data/0', 'fc2.weight/version', 'fc2.weight/.data/serialization_id'])\n",
      "Layer: conv1.bias, Keys: dict_keys(['conv1.bias/data.pkl', 'conv1.bias/byteorder', 'conv1.bias/data/0', 'conv1.bias/version', 'conv1.bias/.data/serialization_id'])\n",
      "Layer: fc2.bias, Keys: dict_keys(['fc2.bias/data.pkl', 'fc2.bias/byteorder', 'fc2.bias/data/0', 'fc2.bias/version', 'fc2.bias/.data/serialization_id'])\n"
     ]
    }
   ],
   "source": [
    "for layer_name, params in weights.items():\n",
    "    print(f\"Layer: {layer_name}, Keys: {params.keys()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
