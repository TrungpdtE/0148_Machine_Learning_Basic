{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_12522/224685720.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('traffic_sign_model.pth'))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'perimeter_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 259\u001b[0m\n\u001b[1;32m    255\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    258\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo1.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Đường dẫn tới video\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Bắt đầu xử lý video\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 226\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Phát hiện biển báo trong frame\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m traffic_signs \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_traffic_signs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# Xử lý từng biển báo được phát hiện\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sign \u001b[38;5;129;01min\u001b[39;00m traffic_signs:\n",
      "Cell \u001b[0;32mIn[15], line 200\u001b[0m, in \u001b[0;36mdetect_traffic_signs\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    197\u001b[0m traffic_signs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m contour \u001b[38;5;129;01min\u001b[39;00m contours:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdetect_circle_red\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontour\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# Kiểm tra hình tròn đỏ\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         x, y, w, h \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mboundingRect(contour)\n\u001b[1;32m    202\u001b[0m         traffic_signs\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCircle Red\u001b[39m\u001b[38;5;124m'\u001b[39m, x, y, w, h, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRed circle sign\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[15], line 165\u001b[0m, in \u001b[0;36mdetect_circle_red\u001b[0;34m(contour)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dk \u001b[38;5;129;01min\u001b[39;00m dks:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Kiểm tra sự tồn tại của các khóa trước khi truy cập\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspect_ratio_range\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dk:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircularity_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m circularity \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcircularity_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    164\u001b[0m             dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspect_ratio_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m aspect_ratio \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspect_ratio_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m             dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m h \u001b[38;5;241m<\u001b[39m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m perimeter \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mperimeter_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_range\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dk \u001b[38;5;129;01mand\u001b[39;00m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m area \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperimeter_range\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dk \u001b[38;5;129;01mand\u001b[39;00m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperimeter_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m perimeter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m dk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperimeter_range\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'perimeter_min'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Định nghĩa lại lớp mô hình để khôi phục mô hình đã lưu\n",
    "class TrafficSignModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficSignModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 43)  # Cập nhật số lớp đầu ra là 43\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Tải mô hình đã huấn luyện từ tệp\n",
    "model = TrafficSignModel()\n",
    "model.load_state_dict(torch.load('traffic_sign_model.pth'))\n",
    "model.eval()  # Chuyển mô hình sang chế độ evaluation\n",
    "\n",
    "# Định nghĩa các phép biến đổi giống như khi huấn luyện\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hàm dự đoán biển báo từ ảnh\n",
    "def predict_traffic_sign(image):\n",
    "    image = Image.fromarray(image)  # Chuyển đổi từ numpy array sang PIL image\n",
    "    image = transform(image).unsqueeze(0)  # Áp dụng phép biến đổi và thêm batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Dự đoán\n",
    "        _, predicted = torch.max(output, 1)  # Lấy nhãn dự đoán\n",
    "\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "# Các nhãn biển báo giao thông\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    3: 'Canh bao co tre em',\n",
    "    4: 'Cam dau xe',\n",
    "    5: 'Di cham thoi',\n",
    "    6: 'Cam dung va do xe',\n",
    "    7: 'Huong di theo vach ke duong',\n",
    "    8: 'Gap khuc phai',\n",
    "    9: 'Huong phai di vung phai'\n",
    "    # Thêm đủ các nhãn ở đây\n",
    "}\n",
    "\n",
    "\n",
    "# Hàm phát hiện hình tròn đỏ\n",
    "def detect_circle_red(contour):  # Kiểm tra hình tròn đỏ\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area < 500: return False\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    if perimeter == 0: return False\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    aspect_ratio = float(w) / h\n",
    "    circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "\n",
    "    # Điều kiện cho hình tròn đỏ hợp lệ\n",
    "    dks = [\n",
    "        {\"circularity_range\": (0.65, 1), \"aspect_ratio_range\": (0.75, 1.2), \"height_range\": (37, 120), \"perimeter_min\": 175},\n",
    "        {\"circularity_range\": (0.23, 0.24), \"aspect_ratio_range\": (0.75, 1.2), \"height_range\": (37, 120), \"area_range\": (595, 700), \"perimeter_range\": (165, 180)},\n",
    "        {\"circularity_range\": (0.12, 0.13), \"area_range\": (1590, 1658), \"perimeter_range\": (390, 420)}\n",
    "    ]\n",
    "    for dk in dks:\n",
    "        if (dk[\"circularity_range\"][0] <= circularity <= dk[\"circularity_range\"][1] and\n",
    "            dk[\"aspect_ratio_range\"][0] <= aspect_ratio <= dk[\"aspect_ratio_range\"][1] and\n",
    "            dk[\"height_range\"][0] < h < dk[\"height_range\"][1] and perimeter >= dk[\"perimeter_min\"] and\n",
    "            dk[\"area_range\"][0] <= area <= dk[\"area_range\"][1] and\n",
    "            dk[\"perimeter_range\"][0] <= perimeter <= dk[\"perimeter_range\"][1]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Hàm phát hiện hình tròn xanh\n",
    "def detect_circle_blue(contour):  # Kiểm tra hình tròn xanh\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area < 2300: return False\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    if perimeter == 0: return False\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    aspect_ratio = float(w) / h\n",
    "    circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "\n",
    "    # Điều kiện cho hình tròn xanh hợp lệ\n",
    "    small_circle = 0.67 <= circularity <= 1 and 0.9 <= aspect_ratio <= 1.2 and 37 < h < 150\n",
    "    medium_circle = 0.36 <= circularity < 0.67 and 0.9 <= aspect_ratio <= 1.2 and 37 < h < 150 and area > 8500 and perimeter > 500\n",
    "    large_circle = 0.25 <= circularity < 0.36 and 0.9 <= aspect_ratio <= 1.2 and 37 < h < 150 and area > 14500 and perimeter > 700\n",
    "\n",
    "    if small_circle or medium_circle or large_circle:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Hàm phát hiện hình tam giác đỏ\n",
    "def detect_triangle_red(contour):  # Kiểm tra hình tam giác đỏ\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area < 2300: return False\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    approx = cv2.approxPolyDP(contour, 0.04 * perimeter, True)\n",
    "    if len(approx) == 3:  # Hình tam giác có 3 cạnh\n",
    "        x, y, w, h = cv2.boundingRect(approx)\n",
    "        aspect_ratio = float(w) / h\n",
    "        if area < 1400 and perimeter < 150: return False\n",
    "        if 0.9 < aspect_ratio < 1 and 30 < w < 150 and 30 < h < 150:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Hàm phát hiện hình chữ nhật xanh\n",
    "def detect_rectangle_blue(contour):  # Kiểm tra hình chữ nhật xanh\n",
    "    area = cv2.contourArea(contour)\n",
    "    if area < 1700: return False\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    approx = cv2.approxPolyDP(contour, 0.02 * perimeter, True)\n",
    "    if len(approx) != 4: return False\n",
    "    x, y, w, h = cv2.boundingRect(approx)\n",
    "    aspect_ratio = float(w) / h\n",
    "    if 44 < w < 90 and 32 < h < 60 and 1200 < area < 6000 and 140 < perimeter < 400:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Hàm phát hiện hình tròn xanh\n",
    "def detect_circle_red(contour):  # Kiểm tra hình tròn đỏ\n",
    "    # Tính các đặc trưng hình học\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    aspect_ratio = float(w) / h\n",
    "    area = cv2.contourArea(contour)\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    \n",
    "    # Kiểm tra độ tròn\n",
    "    circularity = 4 * np.pi * (area / (perimeter ** 2)) if perimeter > 0 else 0\n",
    "\n",
    "    dks = [\n",
    "        {\"circularity_range\": (0.65, 1), \"aspect_ratio_range\": (0.75, 1.2), \"height_range\": (37, 120), \"perimeter_min\": 175},\n",
    "        {\"circularity_range\": (0.23, 0.24), \"aspect_ratio_range\": (0.75, 1.2), \"height_range\": (37, 120), \"area_range\": (595, 700), \"perimeter_range\": (165, 180)},\n",
    "        {\"circularity_range\": (0.12, 0.13), \"area_range\": (1590, 1658), \"perimeter_range\": (390, 420)}\n",
    "    ]\n",
    "    \n",
    "    for dk in dks:\n",
    "        # Kiểm tra sự tồn tại của các khóa trước khi truy cập\n",
    "        if \"aspect_ratio_range\" in dk:\n",
    "            if (dk[\"circularity_range\"][0] <= circularity <= dk[\"circularity_range\"][1] and\n",
    "                dk[\"aspect_ratio_range\"][0] <= aspect_ratio <= dk[\"aspect_ratio_range\"][1] and\n",
    "                dk[\"height_range\"][0] < h < dk[\"height_range\"][1] and perimeter >= dk[\"perimeter_min\"] and\n",
    "                \"area_range\" in dk and dk[\"area_range\"][0] <= area <= dk[\"area_range\"][1] and\n",
    "                \"perimeter_range\" in dk and dk[\"perimeter_range\"][0] <= perimeter <= dk[\"perimeter_range\"][1]):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Hàm xử lý ảnh\n",
    "def preprocess_image(image):  # Xử lý ảnh\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_red1 = np.array([0, 150, 50])\n",
    "    upper_red1 = np.array([10, 255, 150])\n",
    "    lower_red2 = np.array([150, 100, 20])\n",
    "    upper_red2 = np.array([180, 255, 150])\n",
    "    mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "\n",
    "    lower_blue = np.array([90, 50, 70])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    mask_combined = cv2.bitwise_or(mask_red, mask_blue)\n",
    "    return mask_combined\n",
    "\n",
    "\n",
    "# Hàm phát hiện biển báo giao thông trong ảnh\n",
    "def detect_traffic_signs(image):  # Phát hiện biển báo giao thông\n",
    "    mask = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    traffic_signs = []\n",
    "\n",
    "    for contour in contours:\n",
    "        if detect_circle_red(contour):  # Kiểm tra hình tròn đỏ\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            traffic_signs.append(('Circle Red', x, y, w, h, 'Red circle sign'))\n",
    "        elif detect_circle_blue(contour):  # Kiểm tra hình tròn xanh\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            traffic_signs.append(('Circle Blue', x, y, w, h, 'Blue circle sign'))\n",
    "        elif detect_triangle_red(contour):  # Kiểm tra hình tam giác đỏ\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            traffic_signs.append(('Triangle Red', x, y, w, h, 'Red triangle sign'))\n",
    "        elif detect_rectangle_blue(contour):  # Kiểm tra hình chữ nhật xanh\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            traffic_signs.append(('Rectangle Blue', x, y, w, h, 'Blue rectangle sign'))\n",
    "\n",
    "    return traffic_signs\n",
    "\n",
    "\n",
    "# Quá trình phân loại biển báo trong video\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Phát hiện biển báo trong frame\n",
    "        traffic_signs = detect_traffic_signs(frame)\n",
    "\n",
    "        # Xử lý từng biển báo được phát hiện\n",
    "        for sign in traffic_signs:\n",
    "            label, x, y, w, h, info = sign\n",
    "            sign_image = frame[y:y+h, x:x+w]\n",
    "            sign_image_resized = cv2.resize(sign_image, (64, 64))  # Resize để phù hợp với kích thước đầu vào của model\n",
    "            sign_image_resized = sign_image_resized.astype('float32') / 255.0\n",
    "            sign_image_resized = np.expand_dims(sign_image_resized, axis=0)  # Thêm dimension batch\n",
    "            \n",
    "            # Dự đoán loại biển báo sử dụng model\n",
    "            prediction = model.predict(sign_image_resized)\n",
    "            predicted_label = np.argmax(prediction, axis=1)\n",
    "\n",
    "            # Lấy tên biển báo từ traffic_sign_labels\n",
    "            traffic_sign_name = traffic_sign_labels.get(predicted_label[0], 'Unknown')  # 'Unknown' nếu không có trong từ điển\n",
    "            \n",
    "            # Vẽ bounding box và tên biển báo lên frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Vẽ khung bao biển báo\n",
    "            cv2.putText(frame, traffic_sign_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Hiển thị frame với bounding box và tên biển báo\n",
    "        cv2.imshow('Traffic Sign Detection and Classification', frame)\n",
    "        \n",
    "        # Thoát khi nhấn 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "video_path = 'video1.mp4'  # Đường dẫn tới video\n",
    "process_video(video_path)  # Bắt đầu xử lý video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_8545/2643718048.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('traffic_sign_model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([43, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([43]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tải mô hình đã huấn luyện từ tệp\u001b[39;00m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m TrafficSignModel()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraffic_sign_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Chuyển mô hình sang chế độ evaluation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Định nghĩa các phép biến đổi giống như khi huấn luyện\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([43, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([43])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Định nghĩa lại lớp mô hình để khôi phục mô hình đã lưu\n",
    "class TrafficSignModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficSignModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 43)  # Cập nhật số lớp đầu ra là 43\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Tải mô hình đã huấn luyện từ tệp\n",
    "model = TrafficSignModel()\n",
    "model.load_state_dict(torch.load('traffic_sign_model.pth'))\n",
    "model.eval()  # Chuyển mô hình sang chế độ evaluation\n",
    "\n",
    "# Định nghĩa các phép biến đổi giống như khi huấn luyện\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hàm dự đoán biển báo từ ảnh\n",
    "def predict_traffic_sign(image):\n",
    "    image = Image.fromarray(image)  # Chuyển đổi từ numpy array sang PIL image\n",
    "    image = transform(image).unsqueeze(0)  # Áp dụng phép biến đổi và thêm batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Dự đoán\n",
    "        _, predicted = torch.max(output, 1)  # Lấy nhãn dự đoán\n",
    "\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Phát hiện biển báo trong frame\n",
    "        traffic_signs = detect_traffic_signs(frame)\n",
    "\n",
    "        # Xử lý từng biển báo được phát hiện\n",
    "        for sign in traffic_signs:\n",
    "            # Giải nén các giá trị trả về (label, x, y, w, h, info)\n",
    "            label, x, y, w, h, info = sign  # Điều chỉnh giải nén phù hợp\n",
    "\n",
    "            sign_image = frame[y:y+h, x:x+w]\n",
    "            sign_image_resized = cv2.resize(sign_image, (64, 64))  # Resize để phù hợp với kích thước đầu vào của model\n",
    "            sign_image_resized = sign_image_resized.astype('float32') / 255.0\n",
    "            sign_image_resized = np.expand_dims(sign_image_resized, axis=0)  # Thêm dimension batch\n",
    "            \n",
    "            # Dự đoán loại biển báo sử dụng model\n",
    "            prediction = model.predict(sign_image_resized)\n",
    "            predicted_label = np.argmax(prediction, axis=1)\n",
    "\n",
    "            # Lấy tên biển báo từ traffic_sign_labels\n",
    "            traffic_sign_name = traffic_sign_labels.get(predicted_label[0], 'Unknown')  # 'Unknown' nếu không có trong từ điển\n",
    "            \n",
    "            # Vẽ bounding box và tên biển báo lên frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Vẽ khung bao biển báo\n",
    "            cv2.putText(frame, traffic_sign_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Hiển thị frame với bounding box và tên biển báo\n",
    "        cv2.imshow('Traffic Sign Detection and Classification', frame)\n",
    "        \n",
    "        # Thoát khi nhấn 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Các nhãn biển báo giao thông\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    3: 'Canh bao co tre em',\n",
    "    4: 'Cam dau xe',\n",
    "    5: 'Di cham thoi',\n",
    "    6: 'Cam dung va do xe',\n",
    "    7: 'Huong di theo vach ke duong',\n",
    "    8: 'Gap khuc phai',\n",
    "    9: 'Huong phai di vung phai'\n",
    "    # Thêm đủ các nhãn ở đây\n",
    "}\n",
    "\n",
    "\n",
    "def detect_circle_red(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    #Loai tru cac hinh tron nho, tranh phat hien sai\n",
    "    if area<500:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    \n",
    "    if perimeter == 0:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(contour)\n",
    "    aspect_ratio=float(w)/h\n",
    "    circularity=(4*np.pi*area)/(perimeter**2)\n",
    "    \n",
    "    #Danh sach dieu kien\n",
    "    dks=[\n",
    "        {\n",
    "            \"circularity_range\": (0.65,1),\n",
    "            \"aspect_ratio_range\": (0.75,1.2),\n",
    "            \"height_range\": (37,120),\n",
    "            \"perimeter_min\": 175\n",
    "        },\n",
    "        {\n",
    "            \"circularity_range\": (0.23,0.24),\n",
    "            \"aspect_ratio_range\": (0.75,1.2),\n",
    "            \"height_range\": (37,120),\n",
    "            \"area_range\": (595,700),\n",
    "            \"perimeter_range\": (165,180)\n",
    "        },\n",
    "        {\n",
    "            \"circularity_range\": (0.12,0.13),\n",
    "            \"area_range\": (1590,1658),\n",
    "            \"perimeter_range\": (390,420),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    #kemtra\n",
    "    for dk in dks:\n",
    "        if (dk.get(\"circularity_range\",(0,1))[0]<=circularity<=dk.get(\"circularity_range\",(0,1))[1] and\n",
    "            dk.get(\"aspect_ratio_range\",(0,float(\"inf\")))[0]<=aspect_ratio<=dk.get(\"aspect_ratio_range\",(0,float(\"inf\")))[1] and\n",
    "            dk.get(\"height_range\",(0,float(\"inf\")))[0]<h<dk.get(\"height_range\",(0,float(\"inf\")))[1] and\n",
    "            perimeter>=dk.get(\"perimeter_min\",0) and\n",
    "            dk.get(\"area_range\",(0,float(\"inf\")))[0]<=area<=dk.get(\"area_range\",(0,float(\"inf\")))[1] and\n",
    "            dk.get(\"perimeter_range\",(0,float(\"inf\")))[0]<=perimeter<=dk.get(\"perimeter_range\",(0,float(\"inf\")))[1]):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "#Ham phat hien hinh tron mau xanh\n",
    "def detect_circle_blue(contour):\n",
    "    area=cv2.contourArea(contour)\n",
    "    if area<2300:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    if perimeter == 0:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(contour)\n",
    "    aspect_ratio=float(w)/h\n",
    "    circularity=(4*np.pi*area)/(perimeter**2)\n",
    "\n",
    "    #Dieu kien cho cac loai hinh tron khac nhau\n",
    "    small_circle=0.67<=circularity<=1 and 0.9<=aspect_ratio<=1.2 and 37<h<150\n",
    "    medium_circle=0.36<=circularity<0.67 and 0.9<=aspect_ratio<=1.2 and 37<h<150 and area>8500 and perimeter>500\n",
    "    large_circle=0.25<=circularity<0.36 and 0.9<=aspect_ratio<=1.2 and 37<h<150 and area>14500 and perimeter>700\n",
    "\n",
    "    #Dieu kien loai tru\n",
    "    exclusion_area_perimeter=area<2500 and perimeter<210\n",
    "\n",
    "    #Kiem tra cac dieu kien\n",
    "    if exclusion_area_perimeter:\n",
    "        return False\n",
    "    if small_circle or medium_circle or large_circle:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def detect_triangle_red(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    if area<2300:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,0.04*perimeter,True)\n",
    "\n",
    "    if len(approx) == 3:  #Hinh tam giac co 3 canh\n",
    "        \n",
    "        x,y,w,h=cv2.boundingRect(approx)\n",
    "        aspect_ratio=float(w)/h\n",
    "\n",
    "        #Dieu kien loai tru cho hinh tam giac nho\n",
    "        if area<1400 and perimeter<150:\n",
    "            return False\n",
    "\n",
    "        #Dieu kien cho hinh tam giac hop le\n",
    "        valid_triangle=0.9<aspect_ratio<1 and 30<w<150 and 30<h<150\n",
    "        \n",
    "        if valid_triangle:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_rectangle_blue(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    if area<1700:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,0.02*perimeter,True)\n",
    "\n",
    "    if len(approx)!=4:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(approx)\n",
    "    aspect_ratio=float(w)/h\n",
    "\n",
    "    #Cac dieu kien kich thuoc cho hinh chu nhat mong muon\n",
    "    large_rectangle=w<150 and area>19000\n",
    "    medium_rectangle=44<w<90 and 32<h<60 and 1200<area<6000 and 140<perimeter<400\n",
    "    unwanted_rectangle=95<w<153 and 50<h<86 and perimeter<460 and area<8300\n",
    "\n",
    "    #Loai tru cac truong hop chu vi va dien tich khong phu hop\n",
    "    high_perimeter_exclusion=perimeter>700 and area<10000\n",
    "    low_area_exclusion=perimeter>100 and area<900\n",
    "\n",
    "    #Cac dieu kien ty le khung hinh va kich thuoc\n",
    "    small_aspect_ratio=0.9<aspect_ratio<2 and 20<w<90 and 20<h<185\n",
    "    large_aspect_ratio=0.9<aspect_ratio<2 and 90<w<300 and 20<h<185\n",
    "\n",
    "    #Kiem tra cac dieu kien loai tru va mong muon\n",
    "    if large_rectangle or medium_rectangle:\n",
    "        return True\n",
    "    \n",
    "    if unwanted_rectangle or high_perimeter_exclusion or low_area_exclusion:\n",
    "        return False\n",
    "    \n",
    "    if small_aspect_ratio or large_aspect_ratio:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Hàm xử lý ảnh\n",
    "def preprocess_image(image):  # Xử lý ảnh\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_red1 = np.array([0, 150, 50])\n",
    "    upper_red1 = np.array([10, 255, 150])\n",
    "    lower_red2 = np.array([150, 100, 20])\n",
    "    upper_red2 = np.array([180, 255, 150])\n",
    "    mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "\n",
    "    lower_blue = np.array([90, 50, 70])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    mask_combined = cv2.bitwise_or(mask_red, mask_blue)\n",
    "    return mask_combined\n",
    "\n",
    "\n",
    "# Hàm phát hiện biển báo giao thông trong ảnh\n",
    "def detect_traffic_signs(image):  # Phát hiện biển báo giao thông\n",
    "    mask = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    traffic_signs = []\n",
    "    for contour in contours:\n",
    "        if detect_circle_red(contour):  # Kiểm tra hình tròn đỏ\n",
    "            traffic_signs.append(('circle_red', contour))\n",
    "        elif detect_circle_blue(contour):  # Kiểm tra hình tròn xanh\n",
    "            traffic_signs.append(('circle_blue', contour))\n",
    "        elif detect_triangle_red(contour):  # Kiểm tra hình tam giác đỏ\n",
    "            traffic_signs.append(('triangle_red', contour))\n",
    "        elif detect_rectangle_blue(contour):  # Kiểm tra hình chữ nhật xanh\n",
    "            traffic_signs.append(('rectangle_blue', contour))\n",
    "\n",
    "    return traffic_signs\n",
    "\n",
    "# Main function to run the program\n",
    "video_path = 'video1.mp4'  # Đường dẫn tới video\n",
    "process_video(video_path)  # Bắt đầu xử lý video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_8545/2043796840.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('traffic_sign_model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([43, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([43]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tải mô hình đã huấn luyện từ tệp\u001b[39;00m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m TrafficSignModel()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraffic_sign_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Chuyển mô hình sang chế độ evaluation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Định nghĩa các phép biến đổi giống như khi huấn luyện\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([43, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([43])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# Định nghĩa lại lớp mô hình để khôi phục mô hình đã lưu\n",
    "class TrafficSignModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficSignModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 43)  # Cập nhật số lớp đầu ra là 43\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Tải mô hình đã huấn luyện từ tệp\n",
    "model = TrafficSignModel()\n",
    "model.load_state_dict(torch.load('traffic_sign_model.pth'))\n",
    "model.eval()  # Chuyển mô hình sang chế độ evaluation\n",
    "\n",
    "# Định nghĩa các phép biến đổi giống như khi huấn luyện\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def detect_circle_red(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    #Loai tru cac hinh tron nho, tranh phat hien sai\n",
    "    if area<500:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    \n",
    "    if perimeter == 0:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(contour)\n",
    "    aspect_ratio=float(w)/h\n",
    "    circularity=(4*np.pi*area)/(perimeter**2)\n",
    "    \n",
    "    #Danh sach dieu kien\n",
    "    dks=[\n",
    "        {\n",
    "            \"circularity_range\": (0.65,1),\n",
    "            \"aspect_ratio_range\": (0.75,1.2),\n",
    "            \"height_range\": (37,120),\n",
    "            \"perimeter_min\": 175\n",
    "        },\n",
    "        {\n",
    "            \"circularity_range\": (0.23,0.24),\n",
    "            \"aspect_ratio_range\": (0.75,1.2),\n",
    "            \"height_range\": (37,120),\n",
    "            \"area_range\": (595,700),\n",
    "            \"perimeter_range\": (165,180)\n",
    "        },\n",
    "        {\n",
    "            \"circularity_range\": (0.12,0.13),\n",
    "            \"area_range\": (1590,1658),\n",
    "            \"perimeter_range\": (390,420),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    #kemtra\n",
    "    for dk in dks:\n",
    "        if (dk.get(\"circularity_range\",(0,1))[0]<=circularity<=dk.get(\"circularity_range\",(0,1))[1] and\n",
    "            dk.get(\"aspect_ratio_range\",(0,float(\"inf\")))[0]<=aspect_ratio<=dk.get(\"aspect_ratio_range\",(0,float(\"inf\")))[1] and\n",
    "            dk.get(\"height_range\",(0,float(\"inf\")))[0]<h<dk.get(\"height_range\",(0,float(\"inf\")))[1] and\n",
    "            perimeter>=dk.get(\"perimeter_min\",0) and\n",
    "            dk.get(\"area_range\",(0,float(\"inf\")))[0]<=area<=dk.get(\"area_range\",(0,float(\"inf\")))[1] and\n",
    "            dk.get(\"perimeter_range\",(0,float(\"inf\")))[0]<=perimeter<=dk.get(\"perimeter_range\",(0,float(\"inf\")))[1]):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "#Ham phat hien hinh tron mau xanh\n",
    "def detect_circle_blue(contour):\n",
    "    area=cv2.contourArea(contour)\n",
    "    if area<2300:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    if perimeter == 0:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(contour)\n",
    "    aspect_ratio=float(w)/h\n",
    "    circularity=(4*np.pi*area)/(perimeter**2)\n",
    "\n",
    "    #Dieu kien cho cac loai hinh tron khac nhau\n",
    "    small_circle=0.67<=circularity<=1 and 0.9<=aspect_ratio<=1.2 and 37<h<150\n",
    "    medium_circle=0.36<=circularity<0.67 and 0.9<=aspect_ratio<=1.2 and 37<h<150 and area>8500 and perimeter>500\n",
    "    large_circle=0.25<=circularity<0.36 and 0.9<=aspect_ratio<=1.2 and 37<h<150 and area>14500 and perimeter>700\n",
    "\n",
    "    #Dieu kien loai tru\n",
    "    exclusion_area_perimeter=area<2500 and perimeter<210\n",
    "\n",
    "    #Kiem tra cac dieu kien\n",
    "    if exclusion_area_perimeter:\n",
    "        return False\n",
    "    if small_circle or medium_circle or large_circle:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def detect_triangle_red(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    if area<2300:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,0.04*perimeter,True)\n",
    "\n",
    "    if len(approx) == 3:  #Hinh tam giac co 3 canh\n",
    "        \n",
    "        x,y,w,h=cv2.boundingRect(approx)\n",
    "        aspect_ratio=float(w)/h\n",
    "\n",
    "        #Dieu kien loai tru cho hinh tam giac nho\n",
    "        if area<1400 and perimeter<150:\n",
    "            return False\n",
    "\n",
    "        #Dieu kien cho hinh tam giac hop le\n",
    "        valid_triangle=0.9<aspect_ratio<1 and 30<w<150 and 30<h<150\n",
    "        \n",
    "        if valid_triangle:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_rectangle_blue(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    if area<1700:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,0.02*perimeter,True)\n",
    "\n",
    "    if len(approx)!=4:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(approx)\n",
    "    aspect_ratio=float(w)/h\n",
    "\n",
    "    #Cac dieu kien kich thuoc cho hinh chu nhat mong muon\n",
    "    large_rectangle=w<150 and area>19000\n",
    "    medium_rectangle=44<w<90 and 32<h<60 and 1200<area<6000 and 140<perimeter<400\n",
    "    unwanted_rectangle=95<w<153 and 50<h<86 and perimeter<460 and area<8300\n",
    "\n",
    "    #Loai tru cac truong hop chu vi va dien tich khong phu hop\n",
    "    high_perimeter_exclusion=perimeter>700 and area<10000\n",
    "    low_area_exclusion=perimeter>100 and area<900\n",
    "\n",
    "    #Cac dieu kien ty le khung hinh va kich thuoc\n",
    "    small_aspect_ratio=0.9<aspect_ratio<2 and 20<w<90 and 20<h<185\n",
    "    large_aspect_ratio=0.9<aspect_ratio<2 and 90<w<300 and 20<h<185\n",
    "\n",
    "    #Kiem tra cac dieu kien loai tru va mong muon\n",
    "    if large_rectangle or medium_rectangle:\n",
    "        return True\n",
    "    \n",
    "    if unwanted_rectangle or high_perimeter_exclusion or low_area_exclusion:\n",
    "        return False\n",
    "    \n",
    "    if small_aspect_ratio or large_aspect_ratio:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Hàm dự đoán biển báo từ ảnh\n",
    "def predict_traffic_sign(image):\n",
    "    image = Image.fromarray(image)  # Chuyển đổi từ numpy array sang PIL image\n",
    "    image = transform(image).unsqueeze(0)  # Áp dụng phép biến đổi và thêm batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)  # Dự đoán\n",
    "        _, predicted = torch.max(output, 1)  # Lấy nhãn dự đoán\n",
    "\n",
    "    return predicted.item()\n",
    "\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Phát hiện biển báo trong frame\n",
    "        traffic_signs = detect_traffic_signs(frame)\n",
    "\n",
    "        # Xử lý từng biển báo được phát hiện\n",
    "        for sign in traffic_signs:\n",
    "            # Giải nén các giá trị trả về (label, contour)\n",
    "            sign_type, contour = sign  # Chỉ cần 2 giá trị: loại biển báo và contour\n",
    "\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            sign_image = frame[y:y+h, x:x+w]\n",
    "            sign_image_resized = cv2.resize(sign_image, (64, 64))  # Resize để phù hợp với kích thước đầu vào của model\n",
    "            sign_image_resized = sign_image_resized.astype('float32') / 255.0\n",
    "            sign_image_resized = np.expand_dims(sign_image_resized, axis=0)  # Thêm dimension batch\n",
    "\n",
    "            # Dự đoán loại biển báo sử dụng model\n",
    "            prediction = model(torch.tensor(sign_image_resized).permute(0, 3, 1, 2).float())  # Chuyển đổi tensor về đúng định dạng\n",
    "            predicted_label = torch.argmax(prediction, axis=1).item()\n",
    "\n",
    "            # Lấy tên biển báo từ traffic_sign_labels\n",
    "            traffic_sign_name = traffic_sign_labels.get(predicted_label, 'Unknown')  # 'Unknown' nếu không có trong từ điển\n",
    "            \n",
    "            # Vẽ bounding box và tên biển báo lên frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Vẽ khung bao biển báo\n",
    "            cv2.putText(frame, traffic_sign_name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Hiển thị frame với bounding box và tên biển báo\n",
    "        cv2.imshow('Traffic Sign Detection and Classification', frame)\n",
    "        \n",
    "        # Thoát khi nhấn 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Các nhãn biển báo giao thông\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    3: 'Canh bao co tre em',\n",
    "    4: 'Cam dau xe',\n",
    "    5: 'Di cham thoi',\n",
    "    6: 'Cam dung va do xe',\n",
    "    7: 'Huong di theo vach ke duong',\n",
    "    8: 'Gap khuc phai',\n",
    "    9: 'Huong phai di vung phai'\n",
    "    # Thêm đủ các nhãn ở đây\n",
    "}\n",
    "\n",
    "# Hàm phát hiện biển báo giao thông trong ảnh\n",
    "def detect_traffic_signs(image):  # Phát hiện biển báo giao thông\n",
    "    mask = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    traffic_signs = []\n",
    "    for contour in contours:\n",
    "        if detect_circle_red(contour):  # Kiểm tra hình tròn đỏ\n",
    "            traffic_signs.append(('circle_red', contour))\n",
    "        elif detect_circle_blue(contour):  # Kiểm tra hình tròn xanh\n",
    "            traffic_signs.append(('circle_blue', contour))\n",
    "        elif detect_triangle_red(contour):  # Kiểm tra hình tam giác đỏ\n",
    "            traffic_signs.append(('triangle_red', contour))\n",
    "        elif detect_rectangle_blue(contour):  # Kiểm tra hình chữ nhật xanh\n",
    "            traffic_signs.append(('rectangle_blue', contour))\n",
    "\n",
    "    return traffic_signs\n",
    "\n",
    "# Hàm xử lý ảnh\n",
    "def preprocess_image(image):  # Xử lý ảnh\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_red1 = np.array([0, 150, 50])\n",
    "    upper_red1 = np.array([10, 255, 150])\n",
    "    lower_red2 = np.array([150, 100, 20])\n",
    "    upper_red2 = np.array([180, 255, 150])\n",
    "    mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "\n",
    "    lower_blue = np.array([90, 50, 70])\n",
    "    upper_blue = np.array([140, 255, 255])\n",
    "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    mask_combined = cv2.bitwise_or(mask_red, mask_blue)\n",
    "    return mask_combined\n",
    "\n",
    "\n",
    "# Main function to run the program\n",
    "video_path = 'video1.mp4'  # Đường dẫn tới video\n",
    "process_video(video_path)  # Bắt đầu xử lý video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_8545/1354292571.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('traffic_sign_model.pth')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraffic_sign_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Define traffic sign labels (example)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m traffic_sign_labels \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThang hoac Phai, cam queo Trai\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCam di nguoc chieu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Add more labels here\u001b[39;00m\n\u001b[1;32m     24\u001b[0m }\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the trained model\n",
    "model = torch.load('traffic_sign_model.pth')\n",
    "model.eval()\n",
    "\n",
    "# Define traffic sign labels (example)\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    3: 'Canh bao co tre em',\n",
    "    4: 'Cam dau xe',\n",
    "    5: 'Di cham thoi',\n",
    "    6: 'Cam dung va do xe',\n",
    "    7: 'Huong di theo vach ke duong',\n",
    "    8: 'Gap khuc phai',\n",
    "    9: 'Huong phai di vung phai',\n",
    "    # Add more labels here\n",
    "}\n",
    "\n",
    "# Define the transform for input image preprocessing (matching the model's input size)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL image\n",
    "    transforms.Resize((64, 64)),  # Resize to the model's expected size\n",
    "    transforms.ToTensor(),  # Convert to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize if required\n",
    "])\n",
    "\n",
    "# Clean image function\n",
    "def clean_images():\n",
    "    file_list = os.listdir('./')\n",
    "    for file_name in file_list:\n",
    "        if '.png' in file_name:\n",
    "            os.remove(file_name)\n",
    "\n",
    "# Process image function (preprocessing steps)\n",
    "def preprocess_image(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    lower_red1 = np.array([0, 150, 50])\n",
    "    upper_red1 = np.array([10, 255, 150])\n",
    "    lower_red2 = np.array([150, 100, 20])\n",
    "    upper_red2 = np.array([180, 255, 150])\n",
    "    mask_red1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask_red2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    mask_red = cv2.bitwise_or(mask_red1, mask_red2)\n",
    "\n",
    "    lower_blue = np.array([105, 100, 120])\n",
    "    upper_blue = np.array([110, 255, 255])\n",
    "    mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    mask_red = cv2.GaussianBlur(mask_red, (5, 5), 0)\n",
    "    mask_red = cv2.erode(mask_red, None, iterations=2)\n",
    "    mask_red = cv2.dilate(mask_red, None, iterations=2)\n",
    "\n",
    "    mask_blue = cv2.GaussianBlur(mask_blue, (5, 5), 0)\n",
    "    mask_blue = cv2.erode(mask_blue, None, iterations=2)\n",
    "    mask_blue = cv2.dilate(mask_blue, None, iterations=2)\n",
    "\n",
    "    return mask_red, mask_blue\n",
    "def detect_circle_red(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    #Loai tru cac hinh tron nho, tranh phat hien sai\n",
    "    if area<500:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    \n",
    "    if perimeter == 0:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(contour)\n",
    "    aspect_ratio=float(w)/h\n",
    "    circularity=(4*np.pi*area)/(perimeter**2)\n",
    "    \n",
    "    #Danh sach dieu kien\n",
    "    dks=[\n",
    "        {\n",
    "            \"circularity_range\": (0.65,1),\n",
    "            \"aspect_ratio_range\": (0.75,1.2),\n",
    "            \"height_range\": (37,120),\n",
    "            \"perimeter_min\": 175\n",
    "        },\n",
    "        {\n",
    "            \"circularity_range\": (0.23,0.24),\n",
    "            \"aspect_ratio_range\": (0.75,1.2),\n",
    "            \"height_range\": (37,120),\n",
    "            \"area_range\": (595,700),\n",
    "            \"perimeter_range\": (165,180)\n",
    "        },\n",
    "        {\n",
    "            \"circularity_range\": (0.12,0.13),\n",
    "            \"area_range\": (1590,1658),\n",
    "            \"perimeter_range\": (390,420),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    #kemtra\n",
    "    for dk in dks:\n",
    "        if (dk.get(\"circularity_range\",(0,1))[0]<=circularity<=dk.get(\"circularity_range\",(0,1))[1] and\n",
    "            dk.get(\"aspect_ratio_range\",(0,float(\"inf\")))[0]<=aspect_ratio<=dk.get(\"aspect_ratio_range\",(0,float(\"inf\")))[1] and\n",
    "            dk.get(\"height_range\",(0,float(\"inf\")))[0]<h<dk.get(\"height_range\",(0,float(\"inf\")))[1] and\n",
    "            perimeter>=dk.get(\"perimeter_min\",0) and\n",
    "            dk.get(\"area_range\",(0,float(\"inf\")))[0]<=area<=dk.get(\"area_range\",(0,float(\"inf\")))[1] and\n",
    "            dk.get(\"perimeter_range\",(0,float(\"inf\")))[0]<=perimeter<=dk.get(\"perimeter_range\",(0,float(\"inf\")))[1]):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "#Ham phat hien hinh tron mau xanh\n",
    "def detect_circle_blue(contour):\n",
    "    area=cv2.contourArea(contour)\n",
    "    if area<2300:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    if perimeter == 0:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(contour)\n",
    "    aspect_ratio=float(w)/h\n",
    "    circularity=(4*np.pi*area)/(perimeter**2)\n",
    "\n",
    "    #Dieu kien cho cac loai hinh tron khac nhau\n",
    "    small_circle=0.67<=circularity<=1 and 0.9<=aspect_ratio<=1.2 and 37<h<150\n",
    "    medium_circle=0.36<=circularity<0.67 and 0.9<=aspect_ratio<=1.2 and 37<h<150 and area>8500 and perimeter>500\n",
    "    large_circle=0.25<=circularity<0.36 and 0.9<=aspect_ratio<=1.2 and 37<h<150 and area>14500 and perimeter>700\n",
    "\n",
    "    #Dieu kien loai tru\n",
    "    exclusion_area_perimeter=area<2500 and perimeter<210\n",
    "\n",
    "    #Kiem tra cac dieu kien\n",
    "    if exclusion_area_perimeter:\n",
    "        return False\n",
    "    if small_circle or medium_circle or large_circle:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def detect_triangle_red(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    if area<2300:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,0.04*perimeter,True)\n",
    "\n",
    "    if len(approx) == 3:  #Hinh tam giac co 3 canh\n",
    "        \n",
    "        x,y,w,h=cv2.boundingRect(approx)\n",
    "        aspect_ratio=float(w)/h\n",
    "\n",
    "        #Dieu kien loai tru cho hinh tam giac nho\n",
    "        if area<1400 and perimeter<150:\n",
    "            return False\n",
    "\n",
    "        #Dieu kien cho hinh tam giac hop le\n",
    "        valid_triangle=0.9<aspect_ratio<1 and 30<w<150 and 30<h<150\n",
    "        \n",
    "        if valid_triangle:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_rectangle_blue(contour):\n",
    "    \n",
    "    area=cv2.contourArea(contour)\n",
    "    \n",
    "    if area<1700:\n",
    "        return False\n",
    "\n",
    "    perimeter=cv2.arcLength(contour,True)\n",
    "    approx=cv2.approxPolyDP(contour,0.02*perimeter,True)\n",
    "\n",
    "    if len(approx)!=4:\n",
    "        return False\n",
    "\n",
    "    x,y,w,h=cv2.boundingRect(approx)\n",
    "    aspect_ratio=float(w)/h\n",
    "\n",
    "    #Cac dieu kien kich thuoc cho hinh chu nhat mong muon\n",
    "    large_rectangle=w<150 and area>19000\n",
    "    medium_rectangle=44<w<90 and 32<h<60 and 1200<area<6000 and 140<perimeter<400\n",
    "    unwanted_rectangle=95<w<153 and 50<h<86 and perimeter<460 and area<8300\n",
    "\n",
    "    #Loai tru cac truong hop chu vi va dien tich khong phu hop\n",
    "    high_perimeter_exclusion=perimeter>700 and area<10000\n",
    "    low_area_exclusion=perimeter>100 and area<900\n",
    "\n",
    "    #Cac dieu kien ty le khung hinh va kich thuoc\n",
    "    small_aspect_ratio=0.9<aspect_ratio<2 and 20<w<90 and 20<h<185\n",
    "    large_aspect_ratio=0.9<aspect_ratio<2 and 90<w<300 and 20<h<185\n",
    "\n",
    "    #Kiem tra cac dieu kien loai tru va mong muon\n",
    "    if large_rectangle or medium_rectangle:\n",
    "        return True\n",
    "    \n",
    "    if unwanted_rectangle or high_perimeter_exclusion or low_area_exclusion:\n",
    "        return False\n",
    "    \n",
    "    if small_aspect_ratio or large_aspect_ratio:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Detect traffic signs (you can use your existing functions for detecting red/blue shapes)\n",
    "def detect_traffic_signs(image):\n",
    "    mask_red, mask_blue = preprocess_image(image)\n",
    "    contours_red, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_blue, _ = cv2.findContours(mask_blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    traffic_signs = []\n",
    "    red_bboxes = []\n",
    "\n",
    "    for contour in contours_red:\n",
    "        # Use your circle and triangle detection functions\n",
    "        if detect_circle_red(contour):  # Assuming detect_circle_red is defined as before\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            red_bboxes.append((x, y, w, h))\n",
    "            traffic_signs.append(('Red Circle', x, y, w, h))\n",
    "\n",
    "        elif detect_triangle_red(contour):  # Assuming detect_triangle_red is defined as before\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            red_bboxes.append((x, y, w, h))\n",
    "            traffic_signs.append(('Red Triangle', x, y, w, h))\n",
    "\n",
    "    for contour in contours_blue:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Skip red area to avoid double detection\n",
    "        is_inside_red_area = any(rx <= x <= rx + rw and ry <= y <= ry + rh for rx, ry, rw, rh in red_bboxes)\n",
    "        if not is_inside_red_area:\n",
    "            if detect_circle_blue(contour):  # Assuming detect_circle_blue is defined as before\n",
    "                traffic_signs.append(('Blue Circle', x, y, w, h))\n",
    "            elif detect_rectangle_blue(contour):  # Assuming detect_rectangle_blue is defined as before\n",
    "                traffic_signs.append(('Blue Rectangle', x, y, w, h))\n",
    "\n",
    "    return traffic_signs\n",
    "\n",
    "# Main function to process video\n",
    "def main(video_file):\n",
    "    vidcap = cv2.VideoCapture(video_file)\n",
    "    frame_width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_rate = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    output_file = \"output_video.avi\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        success, frame = vidcap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        traffic_signs = detect_traffic_signs(frame)\n",
    "\n",
    "        for shape, x, y, w, h in traffic_signs:\n",
    "            sign_image = frame[y:y + h, x:x + w]  # Crop the detected sign\n",
    "            sign_image_resized = transform(sign_image)  # Preprocess the image for model input\n",
    "\n",
    "            sign_image_resized = sign_image_resized.unsqueeze(0)  # Add batch dimension\n",
    "            with torch.no_grad():\n",
    "                output = model(sign_image_resized)  # Get model output\n",
    "                _, predicted = torch.max(output, 1)  # Get predicted class\n",
    "\n",
    "            label = traffic_sign_labels.get(predicted.item(), \"Unknown\")\n",
    "            color = (0, 255, 0)  # Green for bounding box\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Write the frame with bounding boxes and labels to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Show the frame with annotations\n",
    "        cv2.imshow('Traffic Sign Detection', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    vidcap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the program with the video file\n",
    "if __name__ == '__main__':\n",
    "    main('video1.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_8545/4008836332.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('traffic_sign_model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 32768]) from checkpoint, the shape in current model is torch.Size([512, 8192]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m TrafficSignModel()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Load the trained weights\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraffic_sign_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Function to predict traffic sign from an image\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 32768]) from checkpoint, the shape in current model is torch.Size([512, 8192])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the model architecture (must match the model architecture used during training)\n",
    "class TrafficSignModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficSignModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Use a dummy tensor to calculate the size of the feature map after convolution and pooling\n",
    "        self.dummy_tensor = torch.zeros(1, 3, 32, 32)  # Example input size\n",
    "        self._initialize_fc_layers()\n",
    "\n",
    "    def _initialize_fc_layers(self):\n",
    "        # Get the size of the output after passing through conv layers\n",
    "        with torch.no_grad():\n",
    "            x = self.pool(torch.relu(self.conv1(self.dummy_tensor)))\n",
    "            x = self.pool(torch.relu(self.conv2(x)))\n",
    "            # Flatten the tensor for the fully connected layer\n",
    "            flattened_size = x.numel()\n",
    "        \n",
    "        # Initialize fc1 with the dynamically calculated input size\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)  # Assuming 10 classes for traffic signs (adjust if necessary)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define traffic sign labels (example)\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    3: 'Canh bao co tre em',\n",
    "    4: 'Cam dau xe',\n",
    "    5: 'Di cham thoi',\n",
    "    6: 'Cam dung va do xe',\n",
    "    7: 'Huong di theo vach ke duong',\n",
    "    8: 'Gap khuc phai',\n",
    "    9: 'Huong phai di vung phai'\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = TrafficSignModel()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('traffic_sign_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to predict traffic sign from an image\n",
    "def predict_traffic_sign(image_path):\n",
    "    # Load the image\n",
    "    test_image = cv2.imread(image_path)  # Example image\n",
    "    if test_image is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return\n",
    "    \n",
    "    # Preprocess the image: resize, transpose, and convert to tensor\n",
    "    test_image = cv2.resize(test_image, (32, 32))  # Resize to match model input size (32x32)\n",
    "    test_image = test_image.transpose((2, 0, 1))  # Change to (C, H, W)\n",
    "    test_image = torch.tensor(test_image, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Run the model on the image\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        output = model(test_image)\n",
    "        _, predicted_class = torch.max(output, 1)  # Get the class with the highest probability\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label = traffic_sign_labels[predicted_class.item()]\n",
    "    print(f'Predicted Traffic Sign: {predicted_label}')\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage: Predict traffic sign for an input image\n",
    "image_path = '00243_00000_000010.png'  # Change this to your image file path\n",
    "predicted_label = predict_traffic_sign(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_8545/2140429191.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('traffic_sign_model.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 32768]) from checkpoint, the shape in current model is torch.Size([512, 8192]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([43, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([43]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m TrafficSignModel()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Load the trained weights\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraffic_sign_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Function to predict traffic sign from an image\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TrafficSignModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([512, 32768]) from checkpoint, the shape in current model is torch.Size([512, 8192]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([10, 512]) from checkpoint, the shape in current model is torch.Size([43, 512]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([10]) from checkpoint, the shape in current model is torch.Size([43])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the model architecture (must match the model architecture used during training)\n",
    "class TrafficSignModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TrafficSignModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        # Use a dummy tensor to calculate the size of the feature map after convolution and pooling\n",
    "        self.dummy_tensor = torch.zeros(1, 3, 32, 32)  # Example input size\n",
    "        self._initialize_fc_layers()\n",
    "\n",
    "    def _initialize_fc_layers(self):\n",
    "        # Get the size of the output after passing through conv layers\n",
    "        with torch.no_grad():\n",
    "            x = self.pool(torch.relu(self.conv1(self.dummy_tensor)))\n",
    "            x = self.pool(torch.relu(self.conv2(x)))\n",
    "            # Flatten the tensor for the fully connected layer\n",
    "            flattened_size = x.numel()  # This will give you the size for fc1\n",
    "        \n",
    "        # Initialize fc1 with the dynamically calculated input size\n",
    "        self.fc1 = nn.Linear(flattened_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 43)  # Assuming 43 classes for traffic signs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define traffic sign labels (example)\n",
    "traffic_sign_labels = {\n",
    "    0: 'Thang hoac Phai, cam queo Trai',\n",
    "    1: 'Cam di nguoc chieu',\n",
    "    2: 'Cam re trai',\n",
    "    3: 'Canh bao co tre em',\n",
    "    4: 'Cam dau xe',\n",
    "    5: 'Di cham thoi',\n",
    "    6: 'Cam dung va do xe',\n",
    "    7: 'Huong di theo vach ke duong',\n",
    "    8: 'Gap khuc phai',\n",
    "    9: 'Huong phai di vung phai',\n",
    "    10: 'Biển báo 10', \n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = TrafficSignModel()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('traffic_sign_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to predict traffic sign from an image\n",
    "def predict_traffic_sign(image_path):\n",
    "    # Load the image\n",
    "    test_image = cv2.imread(image_path)  # Example image\n",
    "    if test_image is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return\n",
    "    \n",
    "    # Preprocess the image: resize, transpose, and convert to tensor\n",
    "    test_image = cv2.resize(test_image, (32, 32))  # Resize to match model input size (32x32)\n",
    "    test_image = test_image.transpose((2, 0, 1))  # Change to (C, H, W)\n",
    "    test_image = torch.tensor(test_image, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Run the model on the image\n",
    "    with torch.no_grad():  # No need to compute gradients during inference\n",
    "        output = model(test_image)\n",
    "        _, predicted_class = torch.max(output, 1)  # Get the class with the highest probability\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_label = traffic_sign_labels[predicted_class.item()]\n",
    "    print(f'Predicted Traffic Sign: {predicted_label}')\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage: Predict traffic sign for an input image\n",
    "image_path = '00243_00000_000010.png'  # Change this to your image file path\n",
    "predicted_label = predict_traffic_sign(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/zck8b8fs0rvdfvltbjxwz55h0000gn/T/ipykernel_8545/3148087043.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('traffic_sign_model.pth')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m     47\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m---> 49\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo1.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Dự đoán loại biển báo\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_traffic_sign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Gọi hàm dự đoán của PyTorch\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Vẽ thông tin lên ảnh (ví dụ: label biển báo)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;28mstr\u001b[39m(label), (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mpredict_traffic_sign\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     21\u001b[0m img \u001b[38;5;241m=\u001b[39m transform(img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Thêm batch dimension\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load PyTorch model\n",
    "model = torch.load('traffic_sign_model.pth')\n",
    "# model.eval()\n",
    "\n",
    "# Định dạng chuyển đổi đầu vào cho mô hình (có thể thay đổi theo mô hình của bạn)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hàm xử lý và dự đoán ảnh\n",
    "def predict_traffic_sign(image):\n",
    "    img = Image.fromarray(image)  # Convert numpy array to PIL image\n",
    "    img = transform(img).unsqueeze(0)  # Thêm batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    return predicted.item()\n",
    "\n",
    "# Xử lý video với OpenCV\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Dự đoán loại biển báo\n",
    "        label = predict_traffic_sign(frame)  # Gọi hàm dự đoán của PyTorch\n",
    "\n",
    "        # Vẽ thông tin lên ảnh (ví dụ: label biển báo)\n",
    "        cv2.putText(frame, str(label), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Hiển thị kết quả\n",
    "        cv2.imshow('Traffic Sign Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "process_video('video1.mp4')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
